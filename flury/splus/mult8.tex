\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{C:/PROGRA~1/R/R-22~1.1/share/texmf/Sweave}
\begin{document}
%\title{Chapter 8}
%\author{Flury Bee}

%\maketitle





\section{S-PLUS instructions for Chapter 8}
%*********************************

\textit{last updated: june 22, 1997}


The main computational ingredient of linear principal component analysis
is the extraction of eigenvectors and eigenvalues of a symmetric matrix.
The numerical methods available for computing eigenvectors and eigenvalues
are by themselves highly interesting, but since we are mostly concerned
about the statistical aspects, we will simply use the highly efficient
procedure available in S-PLUS and not worry about details of the iterative
calculations. The particular S-PLUS function we will use is called EIGEN;
it computes eigenvalues and associated (normalized) eigenvectors of a
symmetric matrix. It would be a good idea to familiarize yourself with
EIGEN before starting this tutorial.

We will use the turtle shell example for illustration. First, read the
data using

\begin{Schunk}
\begin{Sinput}
> rawdat <- matrix(scan("c:/work/book/flury/multidat/tab1_4.dat"), 
+     ncol = 4, byrow = T)
\end{Sinput}
\end{Schunk}

We want to use the data of the 24 male turtles only, and do principal
component analysis on log-transformed data. For numerical convenience,
the log-transformed data are also multiplied by 10.

\begin{Schunk}
\begin{Sinput}
> X <- 10 * log(rawdat[1:24, 2:4])
\end{Sinput}
\end{Schunk}

Compute the usual sample statistics and, for later use, initialize
the values of p (number of variables) and N (number of observations):

\begin{Schunk}
\begin{Sinput}
> xbar <- apply(X, 2, mean)
> S <- var(X)
> p <- dim(X)[2]
> N <- dim(X)[1]
\end{Sinput}
\end{Schunk}

Next follows the computation of eigenvectors and eigenvalues:

\begin{Schunk}
\begin{Sinput}
> eig <- eigen(S, symm = T)
\end{Sinput}
\end{Schunk}
Display eig\$values and eig\$vectors. In the spectral decomposition
theorem, it is convenient to use a diagonal matrix with eigenvalues as
diagonal entries; for numerical purposes it is easier to store the
eigenvalues in a vector (eig\$values). The columns of B are eigenvectors of
S in descending order.
The following are simple manipulations with data matrices; the only
difficulty is that observations are rows of data matrices, while in
the theoretical notation observations are column vectors. Make sure
you have understood the contents of the software instructions for
Section 4.2 before continuing.

By Definition 8.3.1, the transformation from original variables to
principal components is achieved by

\begin{Schunk}
\begin{Sinput}
> U <- sweep(X, 2, t(xbar)) %*% eig$vectors
\end{Sinput}
\end{Schunk}

Using

\begin{Schunk}
\begin{Sinput}
> apply(U, 2, mean)
\end{Sinput}
\begin{Soutput}
[1] -3.017262e-15 -4.949744e-16 -5.560150e-16
\end{Soutput}
\begin{Sinput}
> var(U)
\end{Sinput}
\begin{Soutput}
              [,1]          [,2]         [,3]
[1,]  2.330335e+00 -2.962465e-17 -5.79671e-17
[2,] -2.962465e-17  5.983049e-02  9.93141e-17
[3,] -5.796710e-17  9.931410e-17  3.59836e-02
\end{Soutput}
\end{Schunk}
you may verify that the principal components are centered at zero, and
have a diagonal covariance matrix. The eigenvalues in the vector eig\$values
should appear on the diagonal of the covariance matrix.  Consider now
computing a two-dimensional principal component approximation. Let A2
be the matrix containing the first two eigenvectors as columns, i.e.,

\begin{Schunk}
\begin{Sinput}
> A2 <- eig$vectors[, c(1, 2)]
\end{Sinput}
\end{Schunk}

By Definition 8.3.1, the two-dimensional principal component approximation
of X is

\begin{Schunk}
\begin{Sinput}
> Y2 <- sweep(sweep(X, 2, t(xbar)) %*% A2 %*% t(A2), 2, t(xbar), 
+     FUN = "+")
\end{Sinput}
\end{Schunk}
Verify that Y2 has the same mean vector as X. The covariance matrix of Y2
is most easily obtained using the var operator; but you may find it
instructive to verify the numerical result using Equation 5 (Property 3)
from Section 8.4.

The preceding calculation of the two-dimensional principal component
approximation is not really what you would want to do in practical
applications with a large number of variables. Rather, you would have
only the first two principal components stored, and you would want to
construct Y2 using these rather than the original data X. This would
work as follows: from the original data, compute the first two
principal components by

\begin{Schunk}
\begin{Sinput}
> U2 <- sweep(X, 2, t(xbar)) %*% A2
\end{Sinput}
\end{Schunk}

Then, using equation (34) from Section 8.3, the two-dimensional
principal component approximation can be computed as

\begin{Schunk}
\begin{Sinput}
> Y2 <- sweep(U2 %*% t(A2), 2, t(xbar), FUN = "+")
\end{Sinput}
\end{Schunk}

Verify that this gives you the same results as the first method used.

As seen in Section 8.6, we will often be interested in computing
standard errors associated with the coefficients of the eigenvectors.
Using equation (6), it is not difficult to write a program to do
these calculations. Interestingly, it is possible to compute the
standard errors for a given eigenvector without using any loops, but
this requires some nifty matrix calculations. By equations (2) or (4),
we need a matrix of theta-coefficients. First, compute

\begin{Schunk}
\begin{Sinput}
> Ones <- matrix(1, p, p)
> Lambda <- Ones * eig$values
> t(Lambda) - Lambda
\end{Sinput}
\begin{Soutput}
         [,1]        [,2]        [,3]
[1,] 0.000000 -2.27050421 -2.29435111
[2,] 2.270504  0.00000000 -0.02384690
[3,] 2.294351  0.02384690  0.00000000
\end{Soutput}
\end{Schunk}

This operation yields a square matrix
with value eig\$values[i]-eig\$values[j] in the [i,j]-th position. We
will need the inverse of the square of each entry (except for the
diagonal entries which are zero). For any matrix A, the formula A^(-2)
in S-PLUS will compute a matrix of elementwise squared inverses. But

\begin{Schunk}
\begin{Sinput}
> (t(Lambda) - Lambda)^(-2)
\end{Sinput}
\begin{Soutput}
          [,1]         [,2]         [,3]
[1,]       Inf    0.1939793    0.1899679
[2,] 0.1939793          Inf 1758.4753822
[3,] 0.1899679 1758.4753822          Inf
\end{Soutput}
\end{Schunk}
will give you a matrix with "Inf" on the diagonal because the diagonal
entries are zero. Here is the trick:

\begin{Schunk}
\begin{Sinput}
> Q <- (t(Lambda) - Lambda + diag(p))^(-2) - diag(p)
\end{Sinput}
\end{Schunk}
When you display Q, you will now see a matrix with zeros on the main
diagonal, and entries  1 / (eig$values[i] - eig\$values[j])^2 else. This
calculation will fail if any two entries of eig$values are equal, or if
any entry of eig\$values is exactly zero.

To get the matrix of theta coefficients, you need to premultiply
and postmultiply Q by a diagonal matrix with the eig\$values on the
diagonal. Since eig$values is a vector rather than a matrix, this is
achieved by

\begin{Schunk}
\begin{Sinput}
> Theta1 <- sweep(Q, 2, eig$values, FUN = "*")
> Theta <- Theta1 * eig$values
> Theta
\end{Sinput}
\begin{Soutput}
           [,1]       [,2]       [,3]
[1,] 0.00000000 0.02704558 0.01592954
[2,] 0.02704558 0.00000000 3.78585062
[3,] 0.01592954 3.78585062 0.00000000
\end{Soutput}
\end{Schunk}
Voilà!

Let's compute the standard errors for the first eigenvector. One way to
do this would be to put the entries in the first column of Theta on the
diagonal matrix, say D, and compute \verb+B %*% D %*% t(B)+. A more direct
method is

\begin{Schunk}
\begin{Sinput}
> V <- eig$vectors %*% (Theta[, 1] * t(eig$vectors))
\end{Sinput}
\end{Schunk}
This matrix V corresponds to equation (1) or (3) in Section 8.6, with
index h=1. The standard errors of the coefficients of the first
eigenvector are the square roots of the diagonal entries of V/N:

\begin{Schunk}
\begin{Sinput}
> stdeb1 <- sqrt(diag(V/N))
\end{Sinput}
\end{Schunk}
Display the results to verify that you got the same numerical values as
those given in Example 8.6.1. Finally, the computation of standard errors
of the eigenvalues is very simple, according to equation (7):

\begin{Schunk}
\begin{Sinput}
> stdelam <- sqrt(2/N) * eig$values
\end{Sinput}
\end{Schunk}

The above calculation of standard errors of eigenvector coefficients
can be done separately for each eigenvector. It will then be more
convenient to put these calculations into a loop and store the program
as a S-PLUS function, such as function LPC (Linear Principal
Components) given below. Call the procedure using

\begin{Schunk}
\begin{Sinput}
> results <- lpc(X)
\end{Sinput}
\end{Schunk}
and display the results:

\begin{Schunk}
\begin{Sinput}
> cat("eigenvectors:\n")
\end{Sinput}
\begin{Soutput}
eigenvectors:
\end{Soutput}
\begin{Sinput}
> results[1]
\end{Sinput}
\begin{Soutput}
$eigenvectors
          [,1]       [,2]       [,3]
[1,] 0.6831023 -0.1594791  0.7126974
[2,] 0.5102195 -0.5940118 -0.6219534
[3,] 0.5225392  0.7884900 -0.3244015
\end{Soutput}
\begin{Sinput}
> cat("eigenvalues:\n")
\end{Sinput}
\begin{Soutput}
eigenvalues:
\end{Soutput}
\begin{Sinput}
> results[2]
\end{Sinput}
\begin{Soutput}
$eigenvalues
[1] 2.3303347 0.0598305 0.0359836
\end{Soutput}
\begin{Sinput}
> cat("standard errors for eigenvector coefficients:\n")
\end{Sinput}
\begin{Soutput}
standard errors for eigenvector coefficients:
\end{Soutput}
\begin{Sinput}
> results[3]
\end{Sinput}
\begin{Soutput}
$stdeB
           [,1]      [,2]       [,3]
[1,] 0.01912576 0.2839892 0.06573967
[2,] 0.02558075 0.2476141 0.23628939
[3,] 0.02775717 0.1300310 0.31345355
\end{Soutput}
\begin{Sinput}
> cat("standard errors for eigenvalues:\n")
\end{Sinput}
\begin{Soutput}
standard errors for eigenvalues:
\end{Soutput}
\begin{Sinput}
> results[4]
\end{Sinput}
\begin{Soutput}
$stdelam
[1] 0.67270969 0.01727158 0.01038757
\end{Soutput}
\end{Schunk}

Apply this procedure also to the head dimension data of Example 8.6.2,
to verify the numerical results given in Table 8.6.1.


\end{document}
